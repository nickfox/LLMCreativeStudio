import os
import re
from typing import Dict, List, Tuple

import fitz  # PyMuPDF
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.chains.conversational_retrieval.prompt import CONVERSATIONAL_RETRIEVAL_TEMPLATE
from sentence_transformers import SentenceTransformer

# Set up the embedding model and ChromaDB
embeddings = OpenAIEmbeddings()
vectorstore = Chroma(persist_directory="pdf_vector_db", embedding_function=embeddings)

# Load the `all-mpnet-base-v2` model for generating embeddings
model = SentenceTransformer("all-mpnet-base-v2")

def process_pdf(file_path: str) -> Tuple[str, List[Dict[str, str]]]:
    """Load a PDF file, extract text and metadata, identify sections, and extract 
images/tables with captions."""

    def _extract_text_and_metadata(pdf_path: str) -> dict:
        pdf = fitz.open(pdf_path)
        metadata = {
            "title": pdf.metadata["title"],
            "author": pdf.metadata["author"],
            "producer": pdf.metadata["producer"],
            "creator": pdf.metadata["creator"],
            "created": pdf.metadata["creationdate"],
        }
        text = ""
        for page in pdf:
            text += page.get_text()
        return metadata, text

    def _extract_sections(text: str) -> List[str]:
        sections = re.split("(\n={2,}\s*[A-Za-z0-9 ]+\s*\n+)", text)
        return [s.strip() for s in sections if s.strip()]

    def _extract_image_and_table_captions(pdf: fitz.Document) -> List[Dict[str, str]]:
        captions = []
        for i, page in enumerate(pdf):
            blocks = page.get_text("blocks")
            for block in blocks:
                if block["type"] == 0:  # this is text
                    if "caption" in block["style"].lower():
                        xref = int(block["bbox"][3])
                        img = pdf.get_image_by_xref(xref)
                        if img:
                            caption_type = "Image"
                            caption_text = block["lines"][0]["spans"][0]["text"]
                            image_filename = f"image_{i}.png"
                            with open(image_filename, "wb") as io:
                                io.write(img.pixeldata)
                            captions.append({"caption_type": caption_type, "caption_text": 
caption_text, "filename": image_filename})
                        elif block["lines"][0]["spans"][0]["font"].lower() == "courier new":
                            caption_type = "Table"
                            caption_text = block["lines"][0]["spans"][0]["text"]
                            captions.append({"caption_type": caption_type, "caption_text": 
caption_text})
        return captions

    metadata, raw_text = _extract_text_and_metadata(file_path)
    sections = _extract_sections(raw_text)
    captions = _extract_image_and_table_captions(fitz.open(file_path))

    return metadata, sections, captions

def generate_embeddings_for_chunks(text_splitter: CharacterTextSplitter, model: 
SentenceTransformer, chunks: List[str]) -> None:
    """Generate embeddings for the given text chunks."""
    embeddings = model.encode(text_splitter.split_documents(chunks))
    vectorstore.add_texts(texts=chunks, embeddings=embeddings)

def load_pdf_and_generate_embeddings(file_path: str) -> None:
    """Load a PDF file and generate embeddings for its sections."""
    metadata, sections, captions = process_pdf(file_path)
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)

    # Join sections and captions to create the final chunks
    chunks = [metadata["title"]] + sections[:-1]
    for caption in captions:
        if caption["caption_type"] == "Image":
            chunks.append(f"[IMAGE]: {caption['filename']}")
        elif caption["caption_type"] == "Table":
            chunks.append(f"[TABLE]: {caption['caption_text'].replace('\n', ' ')}")
    chunks += [""]  # Add an empty chunk to ensure the last section is processed

    generate_embeddings_for_chunks(text_splitter, model, chunks)

def interrogate_pdf(query: str) -> str:
    """Interrogate the PDF content using a user query."""

    def _get_chain() -> RetrievalQAWithSourcesChain:
        template = CONVERSATIONAL_RETRIEVAL_TEMPLATE.replace("human:", 
"User:\n").replace("assistant:", "PDF Assistant:\n")
        qa = RetrievalQAWithSourcesChain.from_llm(llm=None, chain_type="stuff", verbose=True, 
memory=None, return_source_documents=True, document_loader=None, embedding=embeddings, 
vectorstore=vectorstore, input_key="question", output_key="answer", 
question_template=template)
        return qa

    qa = _get_chain()
    response = qa({"question": query})
    return response["answer"]

if __name__ == "__main__":
    # Load a PDF file and generate embeddings for its sections
    pdf_file_path = "path/to/your/pdf.pdf"
    load_pdf_and_generate_embeddings(pdf_file_path)

    # Use the generated embeddings to answer user queries
    query = "What is the method used to evaluate the performance of the model?"
    response = interrogate_pdf(query)
    print(response)
